import streamlit as st
from langchain_upstage import ChatUpstage, UpstageEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.vectorstores.utils import filter_complex_metadata
from langchain_core.documents import Document
import numpy as np
import tempfile
import os
import shutil
import requests

st.set_page_config(page_title="Embeddings & RAG", page_icon="ğŸ§®", layout="wide")
st.title("ğŸ§® Embeddings & RAG Pipeline")

api_key = st.sidebar.text_input("Upstage API Key", type="password")

if not api_key:
    st.warning("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    if st.sidebar.button("ğŸ”„ ì´ˆê¸°í™”", use_container_width=True):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()
    
    # ì‚¬ì´ë“œë°”ì— í˜„ì¬ ìƒíƒœ í‘œì‹œ
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“Š í˜„ì¬ ìƒíƒœ")
    if 'vectorstore' in st.session_state:
        st.sidebar.success("âœ… ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ë¨")
        st.sidebar.info(f"ğŸ“„ ë¬¸ì„œ: {len(st.session_state.get('docs', []))}ê°œ í˜ì´ì§€")
        st.sidebar.info(f"âœ‚ï¸ ì²­í¬: {len(st.session_state.get('splits', []))}ê°œ")
    else:
        st.sidebar.warning("âš ï¸ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”")
    
    tab1, tab2, tab3 = st.tabs(["ğŸ“š RAG Pipeline", "ğŸ’¬ ì¼ë°˜ LLM", "ğŸ—„ï¸ Vector DB ë‚´ë¶€"])
    
    with tab1:
        st.markdown("### ğŸ“š RAG Pipeline: ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ")
        st.info("01_chat_completions (ChatUpstage) + 02_document_digitization (Document Parse) + Embeddingsë¥¼ ê²°í•©í•œ RAG ì‹œìŠ¤í…œ")
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.markdown("#### 1ï¸âƒ£ ë¬¸ì„œ ì—…ë¡œë“œ & íŒŒì‹±")
            uploaded_file = st.file_uploader(
                "ë¬¸ì„œ ì„ íƒ",
                type=["pdf", "jpg", "jpeg", "png", "docx", "pptx", "xlsx"],
                help="Document Parseë¡œ ë¬¸ì„œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤"
            )
            
            chunk_size = st.slider("ì²­í¬ í¬ê¸°", 100, 2000, 500, 100)
            chunk_overlap = st.slider("ì²­í¬ ì˜¤ë²„ë©", 0, 500, 100, 50)
            
            with st.expander("âš™ï¸ Document Parse ì˜µì…˜"):
                output_format = st.selectbox(
                    "ì¶œë ¥ í˜•ì‹",
                    ["html", "markdown", "text"],
                    index=1,
                    help="html: HTML íƒœê·¸ | markdown: ë§ˆí¬ë‹¤ìš´ | text: ìˆœìˆ˜ í…ìŠ¤íŠ¸"
                )
                parse_mode = st.selectbox(
                    "ëª¨ë“œ",
                    ["auto", "standard", "enhanced"],
                    help="auto: ìë™ ì„ íƒ | standard: ì¼ë°˜ ë¬¸ì„œ | enhanced: ë³µì¡í•œ í‘œ/ì°¨íŠ¸"
                )
                parse_ocr = st.selectbox(
                    "OCR",
                    ["auto", "force"],
                    help="auto: ì´ë¯¸ì§€ë§Œ OCR | force: ëª¨ë“  íŒŒì¼ OCR"
                )
            
            if uploaded_file and st.button("ğŸ“„ ë¬¸ì„œ íŒŒì‹± & ì„ë² ë”©", type="primary"):
                with st.status("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...", expanded=True) as status:
                    try:
                        # 1. ì„ì‹œ íŒŒì¼ ì €ì¥
                        st.write("âœ… 1/4: íŒŒì¼ ì—…ë¡œë“œ ì¤‘...")
                        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                            tmp_file.write(uploaded_file.getvalue())
                            tmp_path = tmp_file.name
                        st.write(f"âœ… íŒŒì¼ í¬ê¸°: {len(uploaded_file.getvalue()) / 1024:.1f}KB")
                        
                        # 2. Document Parse (API ì§ì ‘ í˜¸ì¶œ)
                        st.write("âœ… 2/4: Document Parse API í˜¸ì¶œ ì¤‘...")
                        
                        with open(tmp_path, 'rb') as f:
                            response = requests.post(
                                'https://api.upstage.ai/v1/document-ai/document-parse',
                                headers={'Authorization': f'Bearer {api_key}'},
                                data={
                                    'ocr': parse_ocr,
                                    'output_formats': f"['{output_format}']",
                                    'mode': parse_mode
                                },
                                files={'document': (uploaded_file.name, f)}
                            )
                        
                        os.unlink(tmp_path)
                        
                        if response.status_code != 200:
                            raise Exception(f"API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                        
                        result = response.json()
                        elements = result.get('elements', [])
                        
                        # LangChain Document ê°ì²´ë¡œ ë³€í™˜ (í˜ì´ì§€ë³„ ë¶„ë¦¬)
                        docs = []
                        pages = {}
                        for elem in elements:
                            page_num = elem.get('page', 1)
                            content = elem.get('content', {}).get(output_format, '')
                            if content:
                                if page_num not in pages:
                                    pages[page_num] = []
                                pages[page_num].append(content)
                        
                        for page_num in sorted(pages.keys()):
                            docs.append(Document(
                                page_content='\n'.join(pages[page_num]),
                                metadata={'page': page_num}
                            ))
                        
                        st.write(f"âœ… {len(docs)}ê°œ í˜ì´ì§€ íŒŒì‹± ì™„ë£Œ (format={output_format}, mode={parse_mode})")
                        
                        # 3. í…ìŠ¤íŠ¸ ë¶„í• 
                        st.write(f"âœ… 3/4: í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘ (chunk_size={chunk_size}, overlap={chunk_overlap})...")
                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=chunk_size,
                            chunk_overlap=chunk_overlap
                        )
                        splits = text_splitter.split_documents(docs)
                        st.write(f"âœ… {len(splits)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
                        
                        # 4. ì„ë² ë”© & ë²¡í„° ì €ì¥ì†Œ
                        st.write("âœ… 4/4: ì„ë² ë”© ìƒì„± & Chroma ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì¤‘...")
                        embeddings = UpstageEmbeddings(api_key=api_key, model="embedding-query")
                        
                        # ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§
                        filtered_splits = filter_complex_metadata(splits)
                        
                        # Chroma ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
                        chroma_dir = tempfile.mkdtemp()
                        vectorstore = Chroma.from_documents(
                            documents=filtered_splits,
                            embedding=embeddings,
                            persist_directory=chroma_dir,
                            collection_metadata={"hnsw:space": "cosine"}
                        )
                        st.write(f"âœ… {len(filtered_splits)}ê°œ ë²¡í„° ì €ì¥ ì™„ë£Œ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)")
                        
                        st.session_state['vectorstore'] = vectorstore
                        st.session_state['docs'] = docs
                        st.session_state['splits'] = splits
                        st.session_state['embeddings'] = embeddings
                        st.session_state['chroma_dir'] = chroma_dir
                        
                        status.update(label="âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!", state="complete")
                    
                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜: {str(e)}")
            
            # íŒŒì‹±ëœ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
            if 'docs' in st.session_state:
                st.markdown("---")
                st.markdown("#### ğŸ“„ íŒŒì‹±ëœ ë¬¸ì„œ")
                for i, doc in enumerate(st.session_state['docs'], 1):
                    with st.expander(f"í˜ì´ì§€ {i} (ê¸¸ì´: {len(doc.page_content)}ì)"):
                        st.text_area(f"í˜ì´ì§€ {i}", doc.page_content, height=200, key=f"parse_doc_{i}")
        
        with col2:
            st.markdown("#### 2ï¸âƒ£ ì§ˆë¬¸ & ë‹µë³€ ìƒì„±")
            
            if 'vectorstore' not in st.session_state:
                st.warning("â¬…ï¸ ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  íŒŒì‹±í•˜ì„¸ìš”")
            else:
                model = st.selectbox("ëª¨ë¸", ["solar-mini", "solar-pro3", "solar-pro2"], index=0)
                temperature = st.slider("Temperature", 0.0, 2.0, 0.3, 0.1)
                top_k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜", 1, 10, 3, 1)
                
                question = st.text_area(
                    "ì§ˆë¬¸ ì…ë ¥",
                    "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                    height=100
                )
                
                if st.button("ğŸ” ë‹µë³€ ìƒì„±", type="primary"):
                    with st.status("ğŸ” RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...", expanded=True) as status:
                        try:
                            # 1. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
                            st.write(f"âœ… 1/3: ë²¡í„° ì €ì¥ì†Œì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ (top_k={top_k})...")
                            vectorstore = st.session_state['vectorstore']
                            results = vectorstore.similarity_search_with_score(question, k=top_k)
                            
                            # ChromaëŠ” ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ë°˜í™˜, ìœ ì‚¬ë„ë¡œ ë³€í™˜ í›„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                            relevant_docs_with_scores = sorted(
                                [(doc, 1 - score) for doc, score in results],
                                key=lambda x: x[1],
                                reverse=True
                            )
                            st.write(f"âœ… {len(relevant_docs_with_scores)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
                            
                            st.session_state['last_relevant_docs_with_scores'] = relevant_docs_with_scores
                            st.session_state['last_question'] = question
                            
                            status.update(label="âœ… ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ", state="running")
                            
                            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                            st.write("âœ… 2/3: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘...")
                            context = "\n\n".join([doc.page_content for doc, score in relevant_docs_with_scores])
                            st.write(f"âœ… ì»¨í…ìŠ¤íŠ¸ ì´ ê¸¸ì´: {len(context)}ì")
                            
                            # 3. LLM ë‹µë³€ ìƒì„±
                            st.write(f"âœ… 3/3: {model} ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„± ì¤‘ (temperature={temperature})...")
                            llm = ChatUpstage(api_key=api_key, model=model, temperature=temperature)
                            
                            prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
                            
                            status.update(label="âœ… ë‹µë³€ ìƒì„± ì¤‘...", state="running")
                            
                            st.markdown("##### ğŸ’¬ ë‹µë³€")
                            response_placeholder = st.empty()
                            full_response = ""
                            
                            for chunk in llm.stream([("human", prompt)]):
                                full_response += chunk.content
                                response_placeholder.markdown(full_response + "â–Œ")
                            
                            response_placeholder.markdown(full_response)
                            status.update(label="âœ… RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!", state="complete")
                        
                        except Exception as e:
                            st.error(f"ì˜¤ë¥˜: {str(e)}")
                
                # ê²€ìƒ‰ëœ ë¬¸ì„œ í‘œì‹œ
                if 'last_relevant_docs_with_scores' in st.session_state:
                    st.markdown("---")
                    st.markdown("#### ğŸ“‘ ê²€ìƒ‰ëœ ë¬¸ì„œ")
                    for i, (doc, score) in enumerate(st.session_state['last_relevant_docs_with_scores'], 1):
                        with st.expander(f"ë¬¸ì„œ {i} (ê¸¸ì´: {len(doc.page_content)}ì, ìœ ì‚¬ë„: {score:.4f})"):
                            st.caption(f"ğŸ“Š ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {score:.4f} (ë†’ì„ìˆ˜ë¡ ë” ìœ ì‚¬)")
                            st.text_area(f"ë¬¸ì„œ {i}", doc.page_content, height=200, key=f"search_doc_{i}")
    
    with tab2:
        st.markdown("### ğŸ’¬ ì¼ë°˜ LLM: RAG ì—†ì´ ì§ˆë¬¸í•˜ê¸°")
        st.info("ë¬¸ì„œ ì—†ì´ LLMì˜ ì‚¬ì „ ì§€ì‹ë§Œìœ¼ë¡œ ë‹µë³€")
        
        model_llm = st.selectbox("ëª¨ë¸", ["solar-mini", "solar-pro3", "solar-pro2"], index=0, key="llm_model")
        temperature_llm = st.slider("Temperature", 0.0, 2.0, 0.3, 0.1, key="llm_temp")
        
        question_llm = st.text_area(
            "ì§ˆë¬¸ ì…ë ¥",
            st.session_state.get('last_question', "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"),
            height=100,
            key="llm_question"
        )
        
        if st.button("ğŸ’¬ ë‹µë³€ ìƒì„±", type="primary"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                try:
                    llm = ChatUpstage(api_key=api_key, model=model_llm, temperature=temperature_llm)
                    
                    st.markdown("##### ğŸ’¬ ë‹µë³€")
                    response_placeholder = st.empty()
                    full_response = ""
                    
                    for chunk in llm.stream([("human", question_llm)]):
                        full_response += chunk.content
                        response_placeholder.markdown(full_response + "â–Œ")
                    
                    response_placeholder.markdown(full_response)
                    
                    if 'vectorstore' in st.session_state:
                        st.info("ğŸ’¡ RAG íƒ­ì—ì„œ ê°™ì€ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”. ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜: {str(e)}")
    
    with tab3:
        st.markdown("### ğŸ—„ï¸ Vector DB ë‚´ë¶€ ë“¤ì—¬ë‹¤ë³´ê¸°")
        
        if 'vectorstore' not in st.session_state:
            st.warning("âš ï¸ ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  íŒŒì‹±í•˜ì„¸ìš”")
        else:
            st.success("âœ… Chroma ë²¡í„° ì €ì¥ì†Œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)")
            
            # í†µê³„ ì •ë³´
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ğŸ“„ ì›ë³¸ í˜ì´ì§€", len(st.session_state.get('docs', [])))
            with col2:
                st.metric("âœ‚ï¸ ì²­í¬ ìˆ˜", len(st.session_state.get('splits', [])))
            with col3:
                if 'embeddings' in st.session_state:
                    sample_emb = st.session_state['embeddings'].embed_query("test")
                    st.metric("ğŸ”¢ ë²¡í„° ì°¨ì›", len(sample_emb))
            
            st.markdown("---")
            
            # ì €ì¥ëœ ëª¨ë“  ì²­í¬ ë³´ê¸°
            st.markdown("#### ğŸ“¦ ì €ì¥ëœ ëª¨ë“  ì²­í¬")
            splits = st.session_state.get('splits', [])
            
            for i, split in enumerate(splits, 1):
                with st.expander(f"ì²­í¬ #{i} (ê¸¸ì´: {len(split.page_content)}ì)"):
                    st.text_area(f"ì²­í¬ {i} ë‚´ìš©", split.page_content, height=150, key=f"chunk_{i}")
                    
                    if split.metadata:
                        st.json(split.metadata)
            
            st.markdown("---")
            
            # ë²¡í„° ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
            st.markdown("#### ğŸ§ª ë²¡í„° ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸")
            st.info("ì„ì˜ì˜ ì¿¼ë¦¬ë¡œ ë²¡í„° ì €ì¥ì†Œë¥¼ ì§ì ‘ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
            
            test_query = st.text_input("í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬", "")
            test_k = st.slider("ê²€ìƒ‰í•  ì²­í¬ ìˆ˜", 1, 10, 3, key="test_k")
            
            if test_query and st.button("ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰"):
                vectorstore = st.session_state['vectorstore']
                
                results = vectorstore.similarity_search_with_score(test_query, k=test_k)
                
                # ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ í›„ ì •ë ¬
                results_with_similarity = sorted(
                    [(doc, 1 - score) for doc, score in results],
                    key=lambda x: x[1],
                    reverse=True
                )
                
                st.markdown(f"**ê²€ìƒ‰ ê²°ê³¼: {len(results_with_similarity)}ê°œ ì²­í¬**")
                
                for i, (doc, score) in enumerate(results_with_similarity, 1):
                    with st.expander(f"ê²°ê³¼ {i} (ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {score:.4f})"):
                        st.text_area(f"ë‚´ìš©", doc.page_content, height=150, key=f"test_result_{i}")
                        st.caption(f"ğŸ’¡ ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìŠµë‹ˆë‹¤ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)")
                        if doc.metadata:
                            st.json(doc.metadata)
