import streamlit as st
from langchain_upstage import ChatUpstage

st.set_page_config(page_title="Chat Completions", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Chat Completions (LangChain)")

api_key = st.sidebar.text_input("Upstage API Key", type="password")

# API í‚¤ ê²€ì¦
if not api_key:
    st.warning("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    # ì„¸ì…˜ ì´ˆê¸°í™” ë²„íŠ¼
    if st.sidebar.button("ğŸ”„ ì´ˆê¸°í™”", use_container_width=True):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()
    
    col_left, col_right = st.columns([1, 1])
    
    with col_left:
        st.subheader("âš™ï¸ ì„¤ì • ë° ì‹¤í–‰")
        
        model = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            ["solar-pro3", "solar-pro2", "solar-mini"],
            index=0,
            help="""
            â€¢ solar-pro3: ìµœì‹  ê³ ì„±ëŠ¥ ëª¨ë¸ (solar-pro3-260126)
            â€¢ solar-pro2: ì´ì „ ë²„ì „ ê³ ì„±ëŠ¥ ëª¨ë¸ (solar-pro2-251215)
            â€¢ solar-mini: ë¹ ë¥´ê³  ê²½ëŸ‰í™”ëœ ëª¨ë¸ (solar-mini-250422)
            """
        )
        
        # Reasoning Effort ì„¤ì • (ëª¨ë¸ë³„ ì˜µì…˜ ë‹¤ë¦„)
        reasoning_effort = None
        if model in ["solar-pro3", "solar-pro2"]:
            if model == "solar-pro3":
                options = ["low", "medium", "high"]
                default_idx = 1
                help_text = """
                ë³µì¡í•œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¶”ë¡  ë ˆë²¨ ì œì–´
                
                solar-pro3 (ë™ì  ì¶”ë¡  ì˜ˆì‚°):
                â€¢ high: ë³µì¡í•œ ë¬¸ì œì— ìµœì _reasoning ê°€ëŠ¥
                â€¢ medium: ê· í˜•ì¡íŒ ì¶”ë¡ _reasoning ê°€ëŠ¥ (ê¸°ë³¸ê°’)
                â€¢ low: ì¶”ë¡  ì—†ìŒ, ê°€ì¥ ë¹ ë¥¸ ì‘ë‹µ
                """
            else:
                options = ["minimal", "high"]
                default_idx = 0
                help_text = """
                ë³µì¡í•œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¶”ë¡  ë ˆë²¨ ì œì–´
                
                solar-pro2:
                â€¢ high: ì¶”ë¡  í™œì„±í™”_reasoning ê°€ëŠ¥
                â€¢ minimal: ì¶”ë¡  ë¹„í™œì„±í™”_reasoning ê°€ëŠ¥ (ê¸°ë³¸ê°’)
                
                ì°¸ê³ : mediumì€ highë¡œ, lowëŠ” minimalë¡œ ì·¨ê¸‰ë¨
                """
            
            reasoning_effort = st.selectbox(
                "Reasoning Effort",
                options,
                index=default_idx,
                help=help_text
            )
        
        with st.expander("âš™ï¸ ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°", expanded=True):
            col1, col2 = st.columns(2)
            
            with col1:
                temperature = st.slider(
                    "Temperature",
                    0.0, 2.0, 0.8, 0.1,
                    help="""
                    ìƒ˜í”Œë§ ì˜¨ë„ - ì¶œë ¥ì˜ ë¬´ì‘ìœ„ì„± ì œì–´
                    
                    â€¢ ë†’ì€ ê°’ (0.8): ë” ë¬´ì‘ìœ„ì ì¸ ì¶œë ¥
                    â€¢ ë‚®ì€ ê°’ (0.2): ì§‘ì¤‘ë˜ê³  ê²°ì •ì ì¸ ì¶œë ¥
                    
                    ê¸°ë³¸ê°’: 0.8 (solar-pro3)
                    ë²”ìœ„: 0~2
                    """
                )
                
                top_p = st.slider(
                    "Top P (Nucleus Sampling)",
                    0.0, 1.0, 0.95, 0.05,
                    help="""
                    ëˆ„ì  í™•ë¥  ê¸°ë°˜ í† í° ìƒ˜í”Œë§
                    
                    â€¢ 0.1: ìƒìœ„ 10% í™•ë¥  í† í°ë§Œ ê³ ë ¤
                    â€¢ 0.95: ìƒìœ„ 95% í™•ë¥  í† í° ê³ ë ¤
                    
                    ê¸°ë³¸ê°’: 0.95 (solar-pro3)
                    ë²”ìœ„: 0~1
                    """
                )
            
            with col2:
                frequency_penalty = st.slider(
                    "Frequency Penalty",
                    -2.0, 2.0, 1.1, 0.1,
                    help="""
                    í† í° ë°˜ë³µ ë¹ˆë„ ì œì–´
                    
                    â€¢ ì–‘ìˆ˜ (1.5): ë°˜ë³µ ê°ì†Œ, ë‹¤ì–‘ì„± ì¦ê°€
                    â€¢ 0: í˜ë„í‹° ì—†ìŒ
                    â€¢ ìŒìˆ˜ (-1.0): ë°˜ë³µ í—ˆìš©
                    
                    ê¸°ë³¸ê°’: 1.1
                    ë²”ìœ„: -2.0~2.0
                    """
                )
                
                presence_penalty = st.slider(
                    "Presence Penalty",
                    -2.0, 2.0, 0.0, 0.1,
                    help="""
                    ì´ë¯¸ ë“±ì¥í•œ í† í°ì˜ ì¬ë“±ì¥ ì œì–´
                    
                    â€¢ ì–‘ìˆ˜ (1.5): ìƒˆë¡œìš´ ì£¼ì œë¡œ ìœ ë„
                    â€¢ 0: í˜ë„í‹° ì—†ìŒ
                    â€¢ ìŒìˆ˜ (-1.0): ê¸°ì¡´ í† í° ì¬ì‚¬ìš© ì¥ë ¤
                    
                    frequency_penaltyì™€ ì°¨ì´: ë¹ˆë„ê°€ ì•„ë‹Œ ì¡´ì¬ ì—¬ë¶€ì— ì´ˆì 
                    ê¸°ë³¸ê°’: 0
                    ë²”ìœ„: -2.0~2.0
                    """
                )
        
        with st.expander("ğŸ”§ ìƒì„± ì œì–´", expanded=True):
            max_tokens = st.number_input(
                "Max Tokens",
                min_value=1,
                max_value=32768,
                value=4096,
                help="""
                ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ ì œí•œ
                """
            )
            
            streaming = st.checkbox(
                "ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”",
                value=True,
                help="""
                ì‹¤ì‹œê°„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
                
                â€¢ true: í† í° ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ì „ì†¡ (SSE)
                â€¢ false: ì™„ì„±ëœ ì‘ë‹µì„ í•œ ë²ˆì— ì „ì†¡
                
                ê¸°ë³¸ê°’: false
                """
            )
        
        with st.expander("ğŸ“‹ ê³ ê¸‰ ì„¤ì •", expanded=False):
            response_format_type = st.selectbox(
                "Response Format",
                ["text", "json_object", "json_schema"],
                help="""
                ëª¨ë¸ ì¶œë ¥ í˜•ì‹ ì§€ì •
                
                â€¢ text: ì¼ë°˜ í…ìŠ¤íŠ¸
                â€¢ json_object: JSON í˜•ì‹ (ìŠ¤í‚¤ë§ˆ ì—†ìŒ, í”„ë¡¬í”„íŠ¸ì— 'JSON' í•„ìˆ˜)
                â€¢ json_schema: ì‚¬ìš©ì ì •ì˜ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ JSON (Structured outputs)
                
                í˜¸í™˜ì„±: ëª¨ë“  ëª¨ë¸ ì§€ì›
                """
            )
            
            json_schema_input = None
            if response_format_type == "json_schema":
                json_schema_input = st.text_area(
                    "JSON Schema",
                    '''{"name": "response_schema", "strict": true, "schema": {"type": "object", "properties": {"name": {"type": "string"}}, "required": ["name"], "additionalProperties": false}}''',
                    help="ì¶œë ¥ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” JSON ìŠ¤í‚¤ë§ˆ (name í•„ë“œ í•„ìˆ˜)"
                )
            
            st.divider()
            
            use_tools = st.checkbox(
                "Function Calling í™œì„±í™”",
                value=False,
                help="ì™¸ë¶€ API, ë°ì´í„°ë² ì´ìŠ¤, í•¨ìˆ˜ í˜¸ì¶œ ê¸°ëŠ¥ í™œì„±í™”"
            )
            
            tools_input = None
            tool_choice = "auto"
            parallel_tool_calls = True
            
            if use_tools:
                st.info("ğŸ’¡ í¸ì˜ìƒ mock functionì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” API í˜¸ì¶œì´ë‚˜ DB ì¿¼ë¦¬ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.")
                
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("ğŸŒ¡ï¸ ë‚ ì”¨ API ì˜ˆì‹œ ë¡œë“œ"):
                        st.session_state['tools_example'] = 'weather'
                        st.rerun()
                
                with col2:
                    if st.button("âŒ ì˜ˆì‹œ ì œê±°"):
                        if 'tools_example' in st.session_state:
                            del st.session_state['tools_example']
                        st.rerun()
                
                if st.session_state.get('tools_example') == 'weather':
                    tools_input = '''[
  {
    "type": "function",
    "function": {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          },
          "unit": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"]
          }
        },
        "required": ["location"]
      }
    }
  }
]'''
                    with st.expander("ğŸ“ Tools ì •ì˜ (JSON)", expanded=True):
                        st.code(tools_input, language='json')
                        st.caption("ğŸ”¹ í•¨ìˆ˜: get_current_weather")
                        st.caption("ğŸ”¹ íŒŒë¼ë¯¸í„°: location (í•„ìˆ˜), unit (ì„ íƒ)")
                        st.caption("ğŸ’¡ ì¶”ì²œ ë©”ì‹œì§€: What's the weather like in Seoul?")
                else:
                    tools_input = None
                    st.warning("â¬†ï¸ ìœ„ì—ì„œ ì˜ˆì‹œë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
                
                col1, col2 = st.columns(2)
                with col1:
                    tool_choice = st.selectbox(
                        "Tool Choice",
                        ["auto", "none", "required"],
                        help="""
                        â€¢ auto: ëª¨ë¸ì´ ìë™ ì„ íƒ
                        â€¢ none: í•¨ìˆ˜ í˜¸ì¶œ ì•ˆí•¨
                        â€¢ required: ë°˜ë“œì‹œ í•¨ìˆ˜ í˜¸ì¶œ
                        """
                    )
                
                with col2:
                    if model == "solar-pro3":
                        parallel_tool_calls = st.checkbox(
                            "Parallel Tool Calls",
                            value=True,
                            help="ì—¬ëŸ¬ í•¨ìˆ˜ë¥¼ ë™ì‹œì— í˜¸ì¶œ (solar-pro3 ì „ìš©) - LangChain ë¯¸ì§€ì›"
                        )
                        if parallel_tool_calls:
                            st.warning("âš ï¸ LangChainì—ì„œ parallel_tool_calls íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    else:
                        st.info("Parallel tool calls: solar-pro3 ì „ìš©")
            
            prompt_cache_key = st.text_input(
                "Prompt Cache Key",
                "",
                help="""
                í”„ë¡¬í”„íŠ¸ ìºì‹±ì„ ìœ„í•œ ê³ ìœ  í‚¤
                
                ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë³„ë¡œ ê³ ìœ  í‚¤ ì‚¬ìš© ê¶Œì¥
                ê¸°ë³¸ê°’: null
                """
            )
        
        st.subheader("ğŸ“ ë©”ì‹œì§€ ì…ë ¥")
        system_prompt = st.text_area(
            "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
            "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
            help="AIì˜ ì—­í• , í–‰ë™ ë°©ì‹, ì œì•½ì‚¬í•­ì„ ì •ì˜í•©ë‹ˆë‹¤."
        )
        
        default_user_message = "ì•ˆë…•í•˜ì„¸ìš”!" if response_format_type == "text" else "ì‚¬ìš©ì ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”."
        user_message = st.text_area(
            "ì‚¬ìš©ì ë©”ì‹œì§€",
            default_user_message,
            help="AIì—ê²Œ ì „ë‹¬í•  ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”."
        )
        
        if response_format_type in ["json_object", "json_schema"] and "json" not in user_message.lower():
            st.warning("âš ï¸ JSON ì¶œë ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë©”ì‹œì§€ì— 'JSON' ë‹¨ì–´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        if st.button("ì „ì†¡", type="primary"):
            try:
                # JSON íŒŒì‹± ì‚¬ì „ ê²€ì¦
                import json
                parsed_tools = None
                parsed_schema = None
                
                if use_tools and tools_input:
                    try:
                        parsed_tools = json.loads(tools_input)
                    except json.JSONDecodeError as e:
                        st.error(f"âŒ Tools JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        st.stop()
                
                if response_format_type == "json_schema" and json_schema_input:
                    try:
                        parsed_schema = json.loads(json_schema_input)
                    except json.JSONDecodeError as e:
                        st.error(f"âŒ JSON Schema íŒŒì‹± ì˜¤ë¥˜: {e}")
                        st.stop()
                
                # LLM ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
                llm_params = {
                    "api_key": api_key,
                    "model": model,
                    "temperature": temperature,
                    "top_p": top_p,
                    "frequency_penalty": frequency_penalty,
                    "presence_penalty": presence_penalty,
                    "max_tokens": max_tokens
                }
                
                # ì¶”ê°€ íŒŒë¼ë¯¸í„° (model_kwargs) êµ¬ì„±
                model_kwargs = {}
                if reasoning_effort:
                    if model == "solar-pro3":
                        model_kwargs["reasoning_effort"] = reasoning_effort
                    elif model == "solar-pro2":
                        model_kwargs["reasoning_effort"] = reasoning_effort
                
                if response_format_type == "json_object":
                    model_kwargs["response_format"] = {"type": "json_object"}
                elif response_format_type == "json_schema" and parsed_schema:
                    model_kwargs["response_format"] = {
                        "type": "json_schema",
                        "json_schema": parsed_schema
                    }
                
                # Function Calling ì„¤ì •
                if use_tools and parsed_tools:
                    model_kwargs["tools"] = parsed_tools
                    model_kwargs["tool_choice"] = tool_choice
                
                if prompt_cache_key:
                    model_kwargs["prompt_cache_key"] = prompt_cache_key
                
                if model_kwargs:
                    llm_params["model_kwargs"] = model_kwargs
                
                llm = ChatUpstage(**llm_params)
                
                # ë©”ì‹œì§€ êµ¬ì„±
                messages = [
                    ("system", system_prompt),
                    ("human", user_message)
                ]
                
                # ì‘ë‹µ ë³€ìˆ˜ ì´ˆê¸°í™”
                full_response = ""
                response = None
                
                st.subheader("ğŸ’¬ ì‘ë‹µ:")
                
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (íˆ´ í˜¸ì¶œ ì œì™¸)
                if streaming and not use_tools:
                    with st.spinner("ğŸ”„ ì‘ë‹µ ìƒì„± ì¤‘..."):
                        response_placeholder = st.empty()
                        chunks = []
                        
                        for chunk in llm.stream(messages):
                            chunks.append(chunk)
                            full_response += chunk.content
                            response_placeholder.markdown(full_response + "â–Œ")
                        
                        response_placeholder.markdown(full_response)
                        
                        # ë””ë²„ê¹… ì •ë³´
                        if not full_response:
                            st.error(f"âŒ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ì´ {len(chunks)}ê°œ ì²­í¬ ìˆ˜ì‹ , content ê¸¸ì´: {sum(len(c.content) for c in chunks)}")
                        
                        # ë§ˆì§€ë§‰ ì²­í¬ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                        for i in range(len(chunks)-1, -1, -1):
                            chunk = chunks[i]
                            if hasattr(chunk, 'response_metadata') and chunk.response_metadata:
                                response = chunk
                                break
                else:
                    # ì¼ë°˜ ëª¨ë“œ (invoke)
                    # reasoning ì‚¬ìš© ì—¬ë¶€ í™•ì¸
                    use_reasoning = False
                    if reasoning_effort:
                        if (model == "solar-pro3" and reasoning_effort != "low") or \
                           (model == "solar-pro2" and reasoning_effort != "minimal"):
                            use_reasoning = True
                    
                    spinner_msg = "ğŸ§  ì¶”ë¡  ì¤‘..." if use_reasoning else "ğŸ’¬ ì‘ë‹µ ìƒì„± ì¤‘..."
                    
                    with st.spinner(spinner_msg):
                        response = llm.invoke(messages)
                        full_response = response.content  # invoke ëª¨ë“œì—ì„œë„ ì„¤ì •
                    
                    # Function Calling ì²˜ë¦¬
                    if hasattr(response, 'tool_calls') and response.tool_calls:
                        st.success("ğŸ”§ ëª¨ë¸ì´ í•¨ìˆ˜ í˜¸ì¶œì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.")
                        for i, tc in enumerate(response.tool_calls, 1):
                            with st.expander(f"í•¨ìˆ˜ í˜¸ì¶œ #{i}: {tc.get('name', 'unknown')}"):
                                st.json(tc)
                        
                        # ë”ë¯¸ í•¨ìˆ˜ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜
                        st.divider()
                        st.subheader("ğŸ”„ í•¨ìˆ˜ ì‹¤í–‰ ë° ìµœì¢… ì‘ë‹µ")
                        
                        # Mock í•¨ìˆ˜ ì •ì˜ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” API í˜¸ì¶œë¡œ ëŒ€ì²´)
                        def get_current_weather(location, unit="fahrenheit"):
                            import json
                            if "seoul" in location.lower():
                                return json.dumps({"location": "Seoul", "temperature": "10", "unit": unit})
                            elif "san francisco" in location.lower():
                                return json.dumps({"location": "San Francisco", "temperature": "72", "unit": unit})
                            elif "paris" in location.lower():
                                return json.dumps({"location": "Paris", "temperature": "22", "unit": unit})
                            else:
                                return json.dumps({"location": location, "temperature": "unknown"})
                        
                        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— AI ì‘ë‹µ ì¶”ê°€
                        from langchain_core.messages import AIMessage, ToolMessage
                        messages.append(AIMessage(content="", tool_calls=response.tool_calls))
                        
                        # ê° í•¨ìˆ˜ í˜¸ì¶œ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
                        for tc in response.tool_calls:
                            function_name = tc.get('name')
                            function_args = tc.get('args', {})
                            
                            st.info(f"â–¶ï¸ í•¨ìˆ˜ ì‹¤í–‰: {function_name}({function_args})")
                            
                            # ë”ë¯¸ í•¨ìˆ˜ í˜¸ì¶œ
                            if function_name == "get_current_weather":
                                result = get_current_weather(**function_args)
                                st.code(result, language='json')
                                
                                # ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                                messages.append(ToolMessage(
                                    content=result,
                                    tool_call_id=tc.get('id')
                                ))
                        
                        # í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
                        st.info("ğŸ¤– í•¨ìˆ˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...")
                        
                        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¬ì‚¬ìš© (tools ì œì™¸)
                        final_params = {k: v for k, v in llm_params.items() if k != 'model_kwargs'}
                        final_llm = ChatUpstage(**final_params)
                        
                        if streaming:
                            response_placeholder = st.empty()
                            full_response = ""
                            for chunk in final_llm.stream(messages):
                                full_response += chunk.content
                                response_placeholder.markdown(full_response + "â–Œ")
                            response_placeholder.markdown(full_response)
                        else:
                            final_response = final_llm.invoke(messages)
                            st.markdown(final_response.content)
                    
                    elif response.content:
                        st.markdown(response.content)
                    else:
                        st.warning("ì‘ë‹µ ì»¨í…ì¸ ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í•¨ìˆ˜ í˜¸ì¶œë§Œ ë°œìƒí–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                # ì‘ë‹µ ë©”íƒ€ë°ì´í„° í‘œì‹œ
                with st.expander("ğŸ“ ì „ì²´ ì‘ë‹µ ë¡œê·¸"):
                    if response:
                        # ì›ë³¸ response ê°ì²´ë¥¼ dictë¡œ ë³€í™˜
                        import json
                        
                        # LangChain response ê°ì²´ë¥¼ dictë¡œ ë³€í™˜
                        if hasattr(response, 'dict'):
                            response_dict = response.dict()
                        elif hasattr(response, 'model_dump'):
                            response_dict = response.model_dump()
                        else:
                            response_dict = vars(response)
                        
                        # JSON ì›ë¬¸ í‘œì‹œ
                        st.code(json.dumps(response_dict, indent=2, ensure_ascii=False, default=str), language='json')
                        
                        # reasoning_tokens í‘œì‹œ (í¸ì˜ ê¸°ëŠ¥)
                        metadata = response.response_metadata if hasattr(response, 'response_metadata') else {}
                        token_usage = metadata.get('token_usage')
                        if token_usage:
                            completion_details = token_usage.get('completion_tokens_details', {})
                            reasoning_tokens = completion_details.get('reasoning_tokens')
                            
                            if reasoning_tokens:
                                st.success(f"ğŸ§  reasoning_tokens: {reasoning_tokens:,}")
                    else:
                        st.warning("âš ï¸ ì‘ë‹µ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ë©”íƒ€ë°ì´í„°ë¥¼ ë°›ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        if full_response:
                            st.info(f"ìƒì„±ëœ ì‘ë‹µ: {full_response}")
                    
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    with col_right:
        st.subheader("ğŸ’» ìƒì„±ëœ ì½”ë“œ")
        
        # ì‹¤ì œ íŒŒë¼ë¯¸í„° êµ¬ì„±
        import json
        
        preview_params = {
            "api_key": "YOUR_API_KEY",
            "model": model,
            "temperature": temperature,
            "top_p": top_p,
            "frequency_penalty": frequency_penalty,
            "presence_penalty": presence_penalty,
            "max_tokens": max_tokens
        }
        
        preview_kwargs = {}
        if reasoning_effort:
            if model == "solar-pro3":
                preview_kwargs["reasoning_effort"] = reasoning_effort
            elif model == "solar-pro2":
                preview_kwargs["reasoning_effort"] = reasoning_effort
        
        if response_format_type == "json_object":
            preview_kwargs["response_format"] = {"type": "json_object"}
        elif response_format_type == "json_schema" and json_schema_input:
            try:
                preview_kwargs["response_format"] = {
                    "type": "json_schema",
                    "json_schema": json.loads(json_schema_input)
                }
            except json.JSONDecodeError:
                st.warning("âš ï¸ JSON Schema íŒŒì‹± ì˜¤ë¥˜")
                preview_kwargs["response_format"] = {"type": "json_schema", "json_schema": {}}
        
        if use_tools and tools_input:
            try:
                preview_kwargs["tools"] = json.loads(tools_input)
                preview_kwargs["tool_choice"] = tool_choice
            except json.JSONDecodeError:
                st.warning("âš ï¸ Tools JSON íŒŒì‹± ì˜¤ë¥˜")
        
        if prompt_cache_key:
            preview_kwargs["prompt_cache_key"] = prompt_cache_key
        
        if preview_kwargs:
            preview_params["model_kwargs"] = preview_kwargs
        
        # ì½”ë“œ ìƒì„±
        params_str = json.dumps(preview_params, indent=4, ensure_ascii=False)
        stream_code = "for chunk in llm.stream(messages):\n    print(chunk.content, end='', flush=True)" if streaming else "response = llm.invoke(messages)\nprint(response.content)"
        
        code = f'''from langchain_upstage import ChatUpstage

# íŒŒë¼ë¯¸í„° ì„¤ì •
params = {params_str}

# ChatUpstage ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatUpstage(**params)

# ë©”ì‹œì§€ êµ¬ì„±
messages = [
    ("system", """{system_prompt}"""),
    ("human", """{user_message}""")
]

# {'ìŠ¤íŠ¸ë¦¬ë°' if streaming else 'ì¼ë°˜'} ì‘ë‹µ
{stream_code}
'''
        st.code(code, language='python')
